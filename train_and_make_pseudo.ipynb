{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bitsandbytes.optim import Lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../kaggle_data/\"\n",
    "\n",
    "# Load Training metadata\n",
    "train_landsat_data_path = f\"{DATA_PATH}/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-landsat_time_series/\"\n",
    "train_bioclim_data_path = f\"{DATA_PATH}/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-train-bioclimatic_monthly/\"\n",
    "train_sentinel_data_path=f\"{DATA_PATH}/PA_Train_SatellitePatches_RGB/pa_train_patches_rgb/\"\n",
    "train_metadata_path = f\"{DATA_PATH}/GLC24_PA_metadata_train.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_metadata['speciesId'] = train_metadata['speciesId'].astype(int)\n",
    "\n",
    "# Load Train PO metadata\n",
    "train_po_landsat_data_path = f\"../all_data/SatelliteTimeSeries/cubes/GLC24-PO-train-landsat_time_series/\"\n",
    "train_po_bioclim_data_path = f\"../all_data/EnvironmentalRasters/Climate/Climatic_Monthly_2000-2019_cubes/GLC24_timeseries_csvs/cubes/GLC24-PO-train-bioclimatic_monthly/\"\n",
    "train_po_sentinel_data_path = f\"../all_data/SatellitePatches/po_train_patches_rgb\"\n",
    "train_po_metadata_path = f\"{DATA_PATH}/GLC24_P0_metadata_train.csv\"\n",
    "train_po_metadata = pd.read_csv(train_po_metadata_path)\n",
    "train_po_metadata['speciesId'] = train_po_metadata['speciesId'].astype(int)\n",
    "\n",
    "# Load Test metadata\n",
    "test_landsat_data_path = f\"{DATA_PATH}/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-landsat_time_series/\"\n",
    "test_bioclim_data_path = f\"{DATA_PATH}/TimeSeries-Cubes/TimeSeries-Cubes/GLC24-PA-test-bioclimatic_monthly/\"\n",
    "test_sentinel_data_path = f\"{DATA_PATH}/PA_Test_SatellitePatches_RGB/pa_test_patches_rgb/\"\n",
    "test_metadata_path = f\"{DATA_PATH}/GLC24_PA_metadata_test.csv\"\n",
    "test_metadata = pd.read_csv(test_metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pa_elevation = pd.read_csv(f'{DATA_PATH}/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-train-elevation.csv')\n",
    "train_pa_footprint = pd.read_csv(f'{DATA_PATH}/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-train-human_footprint.csv')\n",
    "train_pa_landcover = pd.read_csv(f'{DATA_PATH}/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-train-landcover.csv')\n",
    "train_pa_soilgrid = pd.read_csv(f'{DATA_PATH}/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-train-soilgrids.csv')\n",
    "\n",
    "train_po_elevation = pd.read_csv('../all_data/EnvironmentalRasters/Elevation/GLC24-PO-train-elevation.csv')\n",
    "train_po_footprint = pd.read_csv('../all_data/EnvironmentalRasters/HumanFootprint/GLC24-PO-train-human-footprint.csv')\n",
    "train_po_landcover = pd.read_csv('../all_data/EnvironmentalRasters/LandCover/GLC24-PO-train-landcover.csv')\n",
    "train_po_soilgrid = pd.read_csv('../all_data/EnvironmentalRasters/Soilgrids/GLC24-PO-train-soilgrids.csv')\n",
    "\n",
    "test_pa_elevation = pd.read_csv(f'{DATA_PATH}/EnvironmentalRasters/EnvironmentalRasters/Elevation/GLC24-PA-test-elevation.csv')\n",
    "test_pa_footprint = pd.read_csv(f'{DATA_PATH}/EnvironmentalRasters/EnvironmentalRasters/Human Footprint/GLC24-PA-test-human_footprint.csv')\n",
    "test_pa_landcover = pd.read_csv(f'{DATA_PATH}/EnvironmentalRasters/EnvironmentalRasters/LandCover/GLC24-PA-test-landcover.csv')\n",
    "test_pa_soilgrid = pd.read_csv(f'{DATA_PATH}/EnvironmentalRasters/EnvironmentalRasters/SoilGrids/GLC24-PA-test-soilgrids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../final_run/valid_species.txt\", \"r\") as f:\n",
    "    valid_species = json.load(f)\n",
    "\n",
    "species_id_to_index = {species_id: i for i, species_id in enumerate(valid_species)}\n",
    "\n",
    "NUM_CLASSES = len(valid_species)\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = train_metadata[train_metadata['speciesId'].isin(species_id_to_index)].reset_index(drop=True)\n",
    "train_po_metadata = train_po_metadata[train_po_metadata['speciesId'].isin(species_id_to_index)].reset_index(drop=True)\n",
    "\n",
    "train_metadata['speciesId'] = train_metadata['speciesId'].map(species_id_to_index)\n",
    "train_po_metadata['speciesId'] = train_po_metadata['speciesId'].map(species_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata['speciesId'].max(), train_po_metadata['speciesId'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pa_rasters = pd.concat([\n",
    "    train_pa_elevation,\n",
    "    train_pa_footprint.drop(columns=['surveyId']),\n",
    "    train_pa_landcover.drop(columns=['surveyId']),\n",
    "    train_pa_soilgrid.drop(columns=['surveyId'])\n",
    "], axis=1)\n",
    "\n",
    "train_po_rasters = pd.concat([\n",
    "    train_po_elevation,\n",
    "    train_po_footprint.drop(columns=['surveyId']),\n",
    "    train_po_landcover.drop(columns=['surveyId']),\n",
    "    train_po_soilgrid.drop(columns=['surveyId'])\n",
    "], axis=1)\n",
    "\n",
    "test_pa_rasters = pd.concat([\n",
    "    test_pa_elevation,\n",
    "    test_pa_footprint.drop(columns=['surveyId']),\n",
    "    test_pa_landcover.drop(columns=['surveyId']),\n",
    "    test_pa_soilgrid.drop(columns=['surveyId'])\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pa_rasters = train_pa_rasters.fillna(train_pa_rasters.mean())\n",
    "# train_po_rasters = train_po_rasters.fillna(train_pa_rasters.mean())\n",
    "# test_pa_rasters = test_pa_rasters.fillna(train_pa_rasters.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pa_dd = train_metadata.drop_duplicates(subset=['surveyId'], ignore_index=True)\n",
    "train_pa_dd = train_pa_dd.drop(columns=['speciesId'])\n",
    "\n",
    "train_pa_species = train_metadata.groupby('surveyId')['speciesId'].apply(list).reset_index()\n",
    "train_pa_dd = pd.merge(train_pa_dd, train_pa_species, on='surveyId', how='left')\n",
    "train_pa_dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_cols = [\n",
    "    'HumanFootprint-Built1994',\n",
    "    'HumanFootprint-Built2009',\n",
    "    'HumanFootprint-croplands1992',\n",
    "    'HumanFootprint-croplands2005',\n",
    "    'HumanFootprint-Lights1994',\n",
    "    'HumanFootprint-Lights2009',\n",
    "    'HumanFootprint-NavWater1994',\n",
    "    'HumanFootprint-NavWater2009',\n",
    "    'HumanFootprint-Pasture1993',\n",
    "    'HumanFootprint-Pasture2009',\n",
    "    'HumanFootprint-Popdensity1990',\n",
    "    'HumanFootprint-Popdensity2010',\n",
    "    'HumanFootprint-Railways',\n",
    "    'HumanFootprint-Roads',\n",
    "]\n",
    "\n",
    "train_pa_rasters = train_pa_rasters.replace([np.inf, -np.inf], np.nan)\n",
    "train_pa_rasters[err_cols] = train_pa_rasters[err_cols].map(lambda x: np.nan if x > 10 or x < 0 else x)\n",
    "train_pa_dd = pd.merge(train_pa_dd, train_pa_rasters, on='surveyId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_min, lat_max = train_pa_dd['lat'].min(), train_pa_dd['lat'].max()\n",
    "lon_min, lon_max = train_pa_dd['lon'].min(), train_pa_dd['lon'].max()\n",
    "\n",
    "train_po_metadata = train_po_metadata[(train_po_metadata['lat'] >= lat_min) & (train_po_metadata['lat'] <= lat_max)]\n",
    "train_po_metadata = train_po_metadata[(train_po_metadata['lon'] >= lon_min) & (train_po_metadata['lon'] <= lon_max)]\n",
    "train_po_metadata = train_po_metadata.drop_duplicates(subset=['surveyId', 'speciesId'], ignore_index=True)\n",
    "train_po_metadata = train_po_metadata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_po_dd = train_po_metadata.drop_duplicates(subset=['surveyId'], ignore_index=True)\n",
    "train_po_dd = train_po_dd.drop(columns=['speciesId'])\n",
    "\n",
    "train_po_species = train_po_metadata.groupby('surveyId')['speciesId'].apply(lambda x: list(x)).reset_index()\n",
    "train_po_dd = pd.merge(train_po_dd, train_po_species, on='surveyId', how='left')\n",
    "train_po_dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_po_rasters = train_po_rasters.replace([np.inf, -np.inf], np.nan)\n",
    "train_po_rasters[err_cols] = train_po_rasters[err_cols].map(lambda x: np.nan if x > 10 or x < 0 else x)\n",
    "train_po_dd = pd.merge(train_po_dd, train_po_rasters, on='surveyId', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_pa_dd), len(train_po_dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 假設 'latitude' 和 'longitude' 是你數據中的經緯度列\n",
    "coordinates = train_pa_dd[['lat', 'lon']]\n",
    "\n",
    "# 設定群組數\n",
    "N = 6000  # 你可以根據需要調整這個值\n",
    "\n",
    "# 執行 K-means 分群\n",
    "cluster = KMeans(n_clusters=N, max_iter=1, random_state=0).fit(coordinates)\n",
    "# cluster = ward_tree(coordinates, n_clusters=N)\n",
    "# cluster = OPTICS(min_samples=50, xi=.05, min_cluster_size=.05).fit(coordinates)\n",
    "\n",
    "# 將分群結果新增為一個新的列\n",
    "train_pa_dd['i_fold'] = cluster.labels_ % 5\n",
    "\n",
    "# 可以打印看看分群結果的一部分\n",
    "train_pa_dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pa_dd['areaInM2'] = train_pa_dd['areaInM2'].replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_COLUMNS = [\n",
    "    'lon', 'lat', 'year', 'geoUncertaintyInM', 'Elevation',\n",
    "\n",
    "    'HumanFootprint-Built1994',\n",
    "    'HumanFootprint-Built2009',\n",
    "    'HumanFootprint-Popdensity2010',\n",
    "    'HumanFootprint-Roads',\n",
    "\n",
    "    'HumanFootprint-croplands1992', 'HumanFootprint-croplands2005',\n",
    "    'HumanFootprint-Lights1994', 'HumanFootprint-Lights2009',\n",
    "    'HumanFootprint-NavWater1994', 'HumanFootprint-NavWater2009',\n",
    "    'HumanFootprint-Pasture1993', 'HumanFootprint-Pasture2009',\n",
    "    'HumanFootprint-Popdensity1990',\n",
    "    \n",
    "    'HumanFootprint-Railways', \n",
    "    'HumanFootprint-HFP1993', 'HumanFootprint-HFP2009', 'LandCover',\n",
    "    'Soilgrid-bdod', 'Soilgrid-cec', 'Soilgrid-cfvo', 'Soilgrid-clay',\n",
    "    'Soilgrid-nitrogen', 'Soilgrid-phh2o', 'Soilgrid-sand', 'Soilgrid-silt',\n",
    "    'Soilgrid-soc',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pa_dd[VALUE_COLUMNS] = train_pa_dd[VALUE_COLUMNS].astype(np.float32)\n",
    "train_po_dd[VALUE_COLUMNS] = train_po_dd[VALUE_COLUMNS].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mean = train_pa_dd[VALUE_COLUMNS].mean()\n",
    "features_std = train_pa_dd[VALUE_COLUMNS].std()\n",
    "\n",
    "train_pa_dd[VALUE_COLUMNS] = (train_pa_dd[VALUE_COLUMNS] - features_mean) / features_std\n",
    "train_pa_dd[VALUE_COLUMNS] = train_pa_dd[VALUE_COLUMNS].fillna(0)\n",
    "\n",
    "train_po_dd[VALUE_COLUMNS] = (train_po_dd[VALUE_COLUMNS] - features_mean) / features_std\n",
    "train_po_dd[VALUE_COLUMNS] = train_po_dd[VALUE_COLUMNS].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_patch_path(data_path, survey_id):\n",
    "    \"\"\"Construct the patch file path based on plot_id as './CD/AB/XXXXABCD.jpeg'\"\"\"\n",
    "    path = data_path\n",
    "    for d in (str(survey_id)[-2:], str(survey_id)[-4:-2]):\n",
    "        path = os.path.join(path, d)\n",
    "\n",
    "    path = os.path.join(path, f\"{survey_id}.jpeg\")\n",
    "\n",
    "    return path\n",
    "    \n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, metadata, bioclimate_path, landsat_path, sentinel_path, split='pa_train'):\n",
    "        self.metadata = metadata\n",
    "        self.bioclimate_path = bioclimate_path\n",
    "        self.landsat_path = landsat_path\n",
    "        self.sentinel_path = sentinel_path\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.sentinel_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406, 0.485], std=[0.229, 0.224, 0.225, 0.229])\n",
    "        ])\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        \n",
    "        if self.split == 'pa_train':\n",
    "            landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_path, f\"GLC24-PA-train-landsat-time-series_{survey_id}_cube.pt\")))\n",
    "            bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclimate_path, f\"GLC24-PA-train-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "        elif self.split == 'pa_test':\n",
    "            landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_path, f\"GLC24-PA-test-landsat_time_series_{survey_id}_cube.pt\")))\n",
    "            bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclimate_path, f\"GLC24-PA-test-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "        elif self.split == 'po_train':\n",
    "            landsat_sample = torch.nan_to_num(torch.load(os.path.join(self.landsat_path, f\"GLC24-PO-train-landsat_time_series_{survey_id}_cube.pt\")))\n",
    "            bioclim_sample = torch.nan_to_num(torch.load(os.path.join(self.bioclimate_path, f\"GLC24-PO-train-bioclimatic_monthly_{survey_id}_cube.pt\")))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid split: {self.split}\")\n",
    "\n",
    "        rgb_sample = np.array(Image.open(construct_patch_path(self.sentinel_path, survey_id)))\n",
    "        nir_sample = np.array(Image.open(construct_patch_path(self.sentinel_path.replace(\"rgb\", \"nir\").replace(\"RGB\", \"NIR\"), survey_id)))\n",
    "        sentinel_sample = np.concatenate((rgb_sample, nir_sample[...,None]), axis=2)\n",
    "        \n",
    "        if isinstance(landsat_sample, torch.Tensor):\n",
    "            landsat_sample = landsat_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            landsat_sample = landsat_sample.numpy()  # Convert tensor to numpy array\n",
    "            \n",
    "        if isinstance(bioclim_sample, torch.Tensor):\n",
    "            bioclim_sample = bioclim_sample.permute(1, 2, 0)  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            bioclim_sample = bioclim_sample.numpy()  # Convert tensor to numpy array   \n",
    "\n",
    "        value_features = self.metadata.iloc[idx][VALUE_COLUMNS].values.astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            landsat_sample = self.transform(landsat_sample)\n",
    "            bioclim_sample = self.transform(bioclim_sample)\n",
    "        \n",
    "        if self.sentinel_transform:\n",
    "            sentinel_sample = self.sentinel_transform(sentinel_sample)\n",
    "\n",
    "        label = []\n",
    "        if 'speciesId' in self.metadata.columns:\n",
    "            species_ids = self.metadata.speciesId[idx]\n",
    "\n",
    "            if self.split == 'pa_train':\n",
    "                label = torch.zeros(NUM_CLASSES)\n",
    "                for species_id in species_ids:\n",
    "                    label_id = species_id\n",
    "                    label[label_id] = 1  # Set the corresponding class index to 1 for each species\n",
    "            elif self.split == 'po_train':\n",
    "                label = torch.full((NUM_CLASSES,), -100.0)\n",
    "                for species_id in species_ids:\n",
    "                    label_id = species_id\n",
    "                    label[label_id] = 1\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid split: {self.split}\")\n",
    "\n",
    "        return value_features, landsat_sample, bioclim_sample, sentinel_sample, label, survey_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pa_dataset = TrainDataset(train_pa_dd, train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path)\n",
    "train_po_dataset = TrainDataset(train_po_dd, train_po_bioclim_data_path, train_po_landsat_data_path, train_po_sentinel_data_path, split='po_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.shape for x in train_po_dataset[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torchgeo.models import ResNet18_Weights, ResNet50_Weights\n",
    "\n",
    "class MultimodalEnsemble(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultimodalEnsemble, self).__init__()\n",
    "\n",
    "        self.value_mlp = nn.Sequential(\n",
    "            nn.Linear(len(VALUE_COLUMNS), 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 512),\n",
    "        )\n",
    "        \n",
    "        self.landsat_norm = nn.LayerNorm([6,4,21])\n",
    "        self.landsat_model = models.resnet18(weights=None, num_classes=0)\n",
    "        # Modify the first convolutional layer to accept 6 channels instead of 3\n",
    "        self.landsat_model.conv1 = nn.Conv2d(6, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.landsat_model.maxpool = nn.Identity()\n",
    "        self.landsat_model.fc = nn.Identity()\n",
    "        \n",
    "        self.bioclim_norm = nn.LayerNorm([4,19,12])\n",
    "        self.bioclim_model = models.resnet18(weights=None, num_classes=0)  \n",
    "        # Modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.bioclim_model.conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bioclim_model.maxpool = nn.Identity()\n",
    "        self.bioclim_model.fc = nn.Identity()\n",
    "\n",
    "        sentinel_weights = ResNet18_Weights.SENTINEL2_RGB_MOCO\n",
    "        self.sentinel_model = timm.create_model(\"resnet18\", in_chans=sentinel_weights.meta[\"in_chans\"], num_classes=0)\n",
    "        self.sentinel_model.load_state_dict(sentinel_weights.get_state_dict(progress=True), strict=False)\n",
    "        with torch.no_grad():\n",
    "            in_conv_w = self.sentinel_model.conv1.weight\n",
    "            in_conv_w_new = torch.cat([in_conv_w, in_conv_w[:, [0]]], dim=1)\n",
    "            self.sentinel_model.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "            self.sentinel_model.conv1.weight = nn.Parameter(in_conv_w_new)\n",
    "\n",
    "        # sentinel_weights = ResNet50_Weights.SENTINEL2_RGB_SECO\n",
    "        # self.sentinel_model = timm.create_model(\"resnet50\", in_chans=sentinel_weights.meta[\"in_chans\"], num_classes=0)\n",
    "        # self.sentinel_model.load_state_dict(sentinel_weights.get_state_dict(progress=True), strict=False)\n",
    "        \n",
    "        self.ln_landsat = nn.LayerNorm(512)\n",
    "        self.ln_bioclim = nn.LayerNorm(512)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(512+512+512+512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, value_features, landsat, bioclim, sentinel):\n",
    "        \n",
    "        value_features = self.value_mlp(value_features)\n",
    "        \n",
    "        landsat = self.landsat_norm(landsat)\n",
    "        landsat = self.landsat_model(landsat)\n",
    "        landsat = self.ln_landsat(landsat)\n",
    "        \n",
    "        bioclim = self.bioclim_norm(bioclim)\n",
    "        bioclim = self.bioclim_model(bioclim)\n",
    "        bioclim = self.ln_bioclim(bioclim)\n",
    "        \n",
    "        sentinel = self.sentinel_model(sentinel)\n",
    "        \n",
    "        x = torch.cat([value_features, landsat, bioclim, sentinel], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_pa_dataset, batch_size=32, shuffle=True, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultimodalEnsemble(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    value_features, landsat, bioclim, sentinel, label, survey_id = batch\n",
    "    with torch.no_grad():\n",
    "        output = model(value_features, landsat, bioclim, sentinel)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "    mixed_precision='bf16',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_multilabel(preds, targets):\n",
    "    \"\"\"\n",
    "    計算 multi-label 的 F1 score (sample wise 平均) 並回傳 recall 和 precision\n",
    "    \n",
    "    Args:\n",
    "        preds (torch.Tensor): 預測值，形狀為 (batch_size, num_classes)，整數類型\n",
    "        targets (torch.Tensor): 實際標籤，形狀為 (batch_size, num_classes)，整數類型\n",
    "        \n",
    "    Returns:\n",
    "        tuple: sample wise 平均的 F1 score, recall, precision\n",
    "    \"\"\"\n",
    "\n",
    "    preds = preds.float()\n",
    "    targets = targets.float()\n",
    "    \n",
    "    # 計算 TP, FP, FN\n",
    "    tp = (preds * targets).sum(dim=1).float()\n",
    "    fp = (preds * (1 - targets)).sum(dim=1).float()\n",
    "    fn = ((1 - preds) * targets).sum(dim=1).float()\n",
    "    \n",
    "    # 計算 precision 和 recall\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    \n",
    "    # 計算 F1 score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "    \n",
    "    # 計算 sample wise 平均的 F1 score, recall, precision\n",
    "    average_f1 = f1.mean().item()\n",
    "    average_precision = precision.mean().item()\n",
    "    average_recall = recall.mean().item()\n",
    "    \n",
    "    return average_f1, average_precision, average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_species_counts = train_metadata['speciesId'].value_counts()\n",
    "pa_rare_species = pa_species_counts[pa_species_counts < 20].index\n",
    "\n",
    "train_po_metadata['is_rare'] = train_po_metadata['speciesId'].isin(pa_rare_species)\n",
    "train_po_sid_have_rare = train_po_metadata.groupby('surveyId')['is_rare'].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in fold_train_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.1, 0.99, 0.05)\n",
    "batch_size = 64\n",
    "\n",
    "for i_fold in range(5):\n",
    "    print(f\"Training fold {i_fold+1}\")\n",
    "\n",
    "    fold_train_metadata = train_pa_dd[train_pa_dd['i_fold'] != i_fold].reset_index(drop=True)\n",
    "    fold_train_dataset = TrainDataset(fold_train_metadata, train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path)\n",
    "    print(f'PA Training data size: {len(fold_train_dataset)}')\n",
    "\n",
    "    # fold_po_metadata = train_po_dd.sample(100000, random_state=i_fold).reset_index(drop=True)\n",
    "    # fold_po_metadata = pd.merge(fold_po_metadata, train_po_sid_have_rare, on='surveyId', how='left')\n",
    "    # fold_po_metadata = fold_po_metadata[fold_po_metadata['is_rare']].reset_index(drop=True)\n",
    "    # train_po_dataset = TrainDataset(fold_po_metadata, train_po_bioclim_data_path, train_po_landsat_data_path, train_po_sentinel_data_path, split='po_train')\n",
    "    # print(f'PO Training data size: {len(train_po_dataset)}')\n",
    "\n",
    "    # fold_train_dataset = torch.utils.data.ConcatDataset([fold_train_dataset, train_po_dataset])\n",
    "    # print(f'Training data size: {len(fold_train_dataset)}')\n",
    "    \n",
    "    fold_train_loader = DataLoader(fold_train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "    fold_val_metadata = train_pa_dd[train_pa_dd['i_fold'] == i_fold].reset_index(drop=True)\n",
    "    fold_val_dataset = TrainDataset(fold_val_metadata, train_bioclim_data_path, train_landsat_data_path, train_sentinel_data_path)\n",
    "    fold_val_loader = DataLoader(fold_val_dataset, batch_size=batch_size*4, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # Check if cuda is available\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        tqdm.write(\"DEVICE = CUDA\")\n",
    "\n",
    "    num_classes = NUM_CLASSES # Number of all unique classes within the PO and PA data.\n",
    "    model = MultimodalEnsemble(num_classes).to(device)\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if \"sentinel\" in name:\n",
    "    #         param.requires_grad = False\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = 0.00025\n",
    "    num_epochs = 10\n",
    "    positive_weigh_factor = 1.0\n",
    "    fold_best_f1 = 0.0\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, verbose=True)\n",
    "\n",
    "    model, optimizer, scheduler = accelerator.prepare(\n",
    "        model, optimizer, scheduler\n",
    "    )\n",
    "\n",
    "    fold_train_loader, fold_val_loader = accelerator.prepare(\n",
    "        fold_train_loader, fold_val_loader\n",
    "    )\n",
    "\n",
    "    tqdm.write(f\"Training for {num_epochs} epochs started.\")\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_f1_scores = {th:[] for th in thresholds}\n",
    "\n",
    "        for batch_idx, (value_features, landsat, bioclim, sentinel, targets, _) in enumerate(tqdm(fold_train_loader, leave=False)):\n",
    "\n",
    "            value_features = value_features.to(device)\n",
    "            landsat = landsat.to(device)\n",
    "            bioclim = bioclim.to(device)\n",
    "            sentinel = sentinel.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(value_features, landsat, bioclim, sentinel)\n",
    "\n",
    "            # label smoothing\n",
    "            # pos_weight = targets*positive_weigh_factor\n",
    "            # criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            \n",
    "            # targets[targets == -100] = 0\n",
    "            # criterion = torch.nn.BCEWithLogitsLoss()\n",
    "            # loss = criterion(outputs, targets)\n",
    "            \n",
    "            output = outputs.reshape(-1)\n",
    "            target = targets.reshape(-1)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                output[target != -100], target[target != -100],\n",
    "                # pos_weight=torch.tensor(3.0).to(device)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 500 == 0:\n",
    "                tqdm.write(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(fold_train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                predictions = (outputs.sigmoid() > threshold)\n",
    "                f1, _, _ = f1_score_multilabel(predictions, (targets==1))\n",
    "                train_f1_scores[threshold].append(f1)\n",
    "\n",
    "        best_threshold, best_f1 = max([(k, np.mean(v)) for k, v in train_f1_scores.items()], key=lambda x: x[1])\n",
    "        tqdm.write(f\"Epoch {epoch+1}. Training best F1 score: {best_f1:.5f} with threshold: {best_threshold:.3f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        print(\"Scheduler:\",scheduler.state_dict())\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        val_f1_scores = {th:[] for th in thresholds}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            all_targets = []\n",
    "            all_outputs = []\n",
    "            for value_features, landsat, bioclim, sentinel, targets, _ in tqdm(fold_val_loader, leave=False):\n",
    "                value_features = value_features.to(device)\n",
    "                landsat = landsat.to(device)\n",
    "                bioclim = bioclim.to(device)\n",
    "                sentinel = sentinel.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(value_features, landsat, bioclim, sentinel)\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    predictions = (outputs.sigmoid() > threshold)\n",
    "                    f1, _, _ = f1_score_multilabel(predictions, targets)\n",
    "                    val_f1_scores[threshold].append(f1)\n",
    "\n",
    "        best_threshold, best_f1 = max([(k, np.mean(v)) for k, v in val_f1_scores.items()], key=lambda x: x[1])\n",
    "        tqdm.write(f\"Epoch {epoch+1}. Validation best F1 score: {best_f1:.5f} with threshold: {best_threshold:.3f}\")\n",
    "\n",
    "        # if best_f1 > fold_best_f1:\n",
    "        #     model.eval()\n",
    "        #     if not os.path.exists(\"all_new_v2_dl\"):\n",
    "        #         os.makedirs(\"all_new_v2_dl\", exist_ok=True)\n",
    "        #     torch.save(model.state_dict(), f\"all_new_v2_dl/model_fold_{i_fold}.pt\")\n",
    "        #     tqdm.write(f\"Saved model with F1 score: {best_f1:.5f}\")\n",
    "\n",
    "        #     fold_best_f1 = best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_freq = train_metadata['speciesId'].value_counts().sort_index().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    for value_features, landsat, bioclim, sentinel, targets, _ in tqdm(fold_val_loader, leave=False):\n",
    "        value_features = value_features.to(device)\n",
    "        landsat = landsat.to(device)\n",
    "        bioclim = bioclim.to(device)\n",
    "        sentinel = sentinel.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(value_features, landsat, bioclim, sentinel)\n",
    "\n",
    "        all_targets.append(targets)\n",
    "        all_outputs.append(outputs)\n",
    "\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "all_outputs = torch.cat(all_outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_freq.max() * 0.000005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(class_freq).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0.0\n",
    "best_threshold = 0.0\n",
    "best_freq_coef = 0.0\n",
    "for threshold in np.arange(0.0, 0.3, 0.01):\n",
    "    for freq_coef in np.arange(0.0, 0.3, 0.01):\n",
    "        real_th = torch.tensor(threshold + freq_coef * np.log(class_freq)).to(device)\n",
    "        predictions = (all_outputs.sigmoid() > real_th)\n",
    "        f1, r, p = f1_score_multilabel(predictions, all_targets)\n",
    "        print(f\"Threshold: {threshold:.3f}, Freq Coef: {freq_coef:.6f}, F1: {f1:.5f}, Recall: {r:.5f}, Precision: {p:.5f}\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_freq_coef = freq_coef\n",
    "    print()\n",
    "\n",
    "print(f\"Best F1: {best_f1:.5f}, Best Threshold: {best_threshold:.3f}, Best Freq Coef: {best_freq_coef:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0.0\n",
    "best_threshold = 0.0\n",
    "best_freq_coef = 0.0\n",
    "for threshold in np.arange(0.0, 0.3, 0.02):\n",
    "    for freq_coef in [0, 0.000001, 0.000002, 0.000003, 0.000004, 0.000005, 0.000007, 0.00001, 0.000015, 0.00002, 0.00003]:\n",
    "        real_th = torch.tensor(threshold + freq_coef * class_freq).to(device)\n",
    "        predictions = (all_outputs.sigmoid() > real_th)\n",
    "        f1, r, p = f1_score_multilabel(predictions, all_targets)\n",
    "        print(f\"Threshold: {threshold:.3f}, Freq Coef: {freq_coef:.6f}, F1: {f1:.5f}, Recall: {r:.5f}, Precision: {p:.5f}\")\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_freq_coef = freq_coef\n",
    "    print()\n",
    "\n",
    "print(f\"Best F1: {best_f1:.5f}, Best Threshold: {best_threshold:.3f}, Best Freq Coef: {best_freq_coef:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pa_rasters = test_pa_rasters.replace([np.inf, -np.inf], np.nan)\n",
    "test_pa_rasters[err_cols] = test_pa_rasters[err_cols].map(lambda x: np.nan if x > 10 or x < 0 else x)\n",
    "test_pa_metadata = pd.merge(test_metadata, test_pa_rasters, on='surveyId', how='left')\n",
    "test_pa_metadata['areaInM2'] = test_pa_metadata['areaInM2'].replace([np.inf, -np.inf], np.nan)\n",
    "test_pa_metadata[VALUE_COLUMNS] = (test_pa_metadata[VALUE_COLUMNS] - features_mean) / features_std\n",
    "test_pa_metadata[VALUE_COLUMNS] = test_pa_metadata[VALUE_COLUMNS].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TrainDataset(\n",
    "    test_pa_metadata, test_bioclim_data_path,\n",
    "    test_landsat_data_path, test_sentinel_data_path, split='pa_test',\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_po_dataset = TrainDataset(train_po_dd, train_po_bioclim_data_path, train_po_landsat_data_path, train_po_sentinel_data_path, split='po_train')\n",
    "train_po_loader = DataLoader(train_po_dataset, batch_size=512, shuffle=False, num_workers=24, pin_memory=True)\n",
    "model = MultimodalEnsemble(NUM_CLASSES).to(device)\n",
    "\n",
    "model, train_po_loader = accelerator.prepare(model, train_po_loader)\n",
    "\n",
    "all_predictions = []\n",
    "for i_fold in tqdm(range(5)):\n",
    "    model.load_state_dict(torch.load(f\"all_new_v2_dl/model_fold_{i_fold}.pt\"))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        survey_ids = []\n",
    "        fold_predictions = []\n",
    "        for batch_idx, (value_features, landsat, bioclim, sentinel, _, survey_id) in enumerate(tqdm(train_po_loader)):\n",
    "            value_features = value_features.to(device)\n",
    "            landsat = landsat.to(device)\n",
    "            bioclim = bioclim.to(device)\n",
    "            sentinel = sentinel.to(device)\n",
    "\n",
    "            outputs = model(value_features, landsat, bioclim, sentinel)\n",
    "            predictions = outputs.sigmoid()\n",
    "            fold_predictions.append(predictions.cpu())\n",
    "            survey_ids.extend(survey_id)\n",
    "\n",
    "            if len(fold_predictions) == 200:\n",
    "                fold_predictions = torch.cat(fold_predictions)\n",
    "                torch.save(fold_predictions, f\"all_new_v2_dl/po_predictions_fold_{i_fold}_batch_{batch_idx+1}.pt\")\n",
    "                fold_predictions = []\n",
    "\n",
    "                print(f\"Saved fold {i_fold} batch {batch_idx} predictions\")\n",
    "\n",
    "        fold_predictions = torch.cat(fold_predictions)\n",
    "        torch.save(fold_predictions, f\"all_new_v2_dl/po_predictions_fold_{i_fold}_batch_{batch_idx}.pt\")\n",
    "        print(f\"Saved fold {i_fold} batch {batch_idx} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batch_indices = list(range(200, 6001, 200)) + [6090]\n",
    "all_pseudo_labels = []\n",
    "\n",
    "for i_batch in tqdm(all_batch_indices):\n",
    "    batch_preds = []\n",
    "    for i_fold in range(5):\n",
    "        pred = torch.load(f\"all_new_v2_dl/po_predictions_fold_{i_fold}_batch_{i_batch}.pt\")\n",
    "        batch_preds.append(pred)\n",
    "    mean_batch_preds = torch.stack(batch_preds, dim=0).mean(dim=0)\n",
    "    \n",
    "    for probs in mean_batch_preds:\n",
    "        pos_indices = torch.where(probs > 0.4)[0].tolist()\n",
    "        ignore_indices = torch.where((probs > 0.05) & (probs <= 0.4))[0].tolist()\n",
    "        all_pseudo_labels.append((pos_indices, ignore_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_label_df = pd.DataFrame()\n",
    "pseudo_label_df['surveyId'] = train_po_dd['surveyId']\n",
    "pseudo_label_df[['pos_indices', 'ignore_indices']] = pd.DataFrame(all_pseudo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_label_df.to_csv(\"all_new_v2_dl/po_pseudo_labels.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
